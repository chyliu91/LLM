# 图解 Transformer 原理
## 全局视角

首先将 Transformer 视为一个黑盒子模型，例如在机器翻译中，模型接受一种语言的输入，输出另一种语言:

![](./imgs/the_transformer_3.png)

进一步打开 Transformer 这个黑盒子模型，我们可以看到，它包含了编码组件和解码组件:

![](./imgs/The_transformer_encoders_decoders.png)

进一步，编码组件是由多个堆叠起来的编码器构成的，解码组件是由多个堆叠起来的解码器构成的:

![](./imgs/The_transformer_encoder_decoder_stack.png)


编码器的结构完全相同（但它们不共享权重）。每个编码器分为两个子层:

![](./imgs/Transformer_encoder.png)

- 编码器的输入首先经过自注意力层，该层可以帮助编码器在编码特定单词时查看输入句子中的其他单词
- 自注意力层的输出被送到前馈神经网络，完全相同的前馈网络独立应用于每个位置

解码器中也包含这两种层，并且在它们之间多加了一个编码-解码注意力层，来帮助解码器关注输入句子的部分:

![](./imgs/Transformer_decoder.png)

## 将张量输入到图中
与一般 NLP 应用的情况一样，我们首先使用嵌入算法(embeding)将每个输入词转换为向量:

![](./imgs/embeddings.png)

词嵌入只针对第一个编码器，其后的编码器的输入为前一个编码器的输出:

![](./imgs/encoder_with_tensors.png)

这里可以看到 Transformer 的一个关键特性: 每个位置上的单词在编码器中流经自己的路径。在自注意力层中，这些路径之间存在依赖关系。然而，前馈层没有这些依赖关系，因此各种路径可以在流经前馈层时并行执行。

更详细的过程可以表示为:

![](./imgs/encoder_with_tensors_2.png)

每个位置上的单词都会经过一个自注意力过程。然后，它们会分别经过一个前馈神经网络——完全相同的网络，每个向量都会分别流过它。


## 自注意力
### 自注意力基本原理
假设以下句子是我们要翻译的输入句子：

`The animal didn't cross the street because it was too tired`。

这句话中的“它”指的是什么？它指的是街道还是动物？对于人类来说，这是一个简单的问题，但对于算法来说却不那么简单。

当模型处理每个单词（输入序列中的每个位置）时，自注意力机制允许它查看输入序列中的其他位置以寻找有助于更好地编码该单词的线索。简单的说就是可以做到联系上下文。

当模型处理`it` 这个词时，自注意力让模型将 `it`与 `animal` 联系起来:

![](./imgs/transformer_self-attention_visualization.png)

### 自注意力计算过程

- 计算自注意力的第一步是从编码器的每个输入向量（在本例中为每个单词的 embeding）创建三个向量。这些向量是通过将 embeding 乘以我们在训练过程中训练的三个矩阵而创建的。对于每个单词，我们创建一个 $q$ 向量、一个 $k$ 向量和一个 $v$ 向量:

![](./imgs/transformer_self_attention_vectors.png)

请注意，这些新向量的维度小于 embeding 向量。这里使用的维度为 64，而 embeding 和编码器输入/输出向量的维度为 512。

- 计算自注意力的第二步是计算分数。假设我们正在计算本例中第一个单词 `Thinking` 的自注意力。我们需要根据这个词对输入句子的每个单词进行评分。分数决定了我们在某个位置编码单词时对输入句子其他部分的关注程度。自注意力得分是通过对 $q$ 向量与我们要评分的相应单词的 $k$ 向量进行点积计算得出的。例如下面计算第一个单词 $x_1$ 的自注意力得分:
    - 计算 $q_1$  与 $k_1$ 的点积
    - 计算 $q_1$  与 $k_2$ 的点积

![](./imgs/transformer_self_attention_score.png)

- 第三步和第四步是将分数除以 8 (论文中使用的关键向量维度(64)的平方根。这会导致更稳定的梯度。这里可能还有其他可能的值，但这是默认值)，然后将结果传递给 Softmax 运算。Softmax 对分数进行归一化，使它们都为正数并且加起来为 1。

![](./imgs/self-attention_softmax.png)

这个 Softmax 分数决定了每个单词在这个位置的表达程度。显然，这个位置的单词将具有最高的 Softmax 分数，但有时关注与当前单词相关的另一个单词也很有用。

- 第五步是将每个 $v$ 向量乘以 Softmax 分数(准备将它们相加)。这里的直觉是保持我们想要关注的单词的值不变，并淹没不相关的单词(例如，通过将它们乘以 0.001 这样的小数字)。

- 第六步是将加权值向量相加。这将产生该位置(第一个单词)的自注意力层的输出。


### 自注意力矩阵计算
第一步是计算 $q$、$k$ 和 $v$ 矩阵。我们将 embeding 打包到矩阵 $X$ 中，并将其乘以我们训练过的权重矩阵($WQ$、$WK$、$WV$)。

![](./imgs/self-attention-matrix-calculation.png)

$X$ 矩阵中的每一行都对应输入句子中的一个单词。


最后，由于我们处理的是矩阵，我们可以将上面第二步到第六步浓缩为一个公式来计算自注意力层的输出:

![](./imgs/self-attention-matrix-calculation-2.png)


### 多头注意力

多头注意力从两个方面提高了注意力层的性能：

- 它扩展了模型关注不同位置的能力。在上面的例子中，$z1$ 包含了其他所有编码的一小部分，但它可能由实际单词本身主导。如果我们翻译 `The animal didn’t cross the street because it was too tired` 这样的句子，将有帮助于更好的理解 `it` 指代什么

- 它为注意层提供了多个“表示子空间”。使用多头注意力，有多组  $WQ$、$WK$、$WV$ 权重矩阵 (Transformer 原论文使用八个注意头，因此我们最终为每个编码器/解码器设置了八组)。这些权重矩阵都是随机初始化的。然后，在训练之后，每组权重矩阵用于将输入嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中

![](./imgs/transformer_attention_heads_qkv.png)


借助多头注意力机制，我们为每个头维护单独的 $WQ$、$WK$、$WV$ 权重矩阵，从而产生不同的 $Q$、$K$、$V$ 矩阵。与之前一样。



# 参考文献
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)